{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('fastai': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c933194f756867ba8dd5da9b0b34dc0031cd583cf07f9dcb657353c4524c172d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# \"A basic Learner\"\n",
    "> \"How to write from scratch a SGD loop\"\n",
    "\n",
    "- badges: true\n",
    "- hide: false\n",
    "- comments: true\n",
    "- author: Andreea Sandu\n",
    "- categories: [fastpages, jupyter, learner, machine learning]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Intro\n",
    "I have recently started the [fastai course v4](https://course.fast.ai/) after many postponements. Although I have been working with AI for a few years now I always thought Jeremy Howard's approach of teaching it's very interesting and ... different. Learning the _whole game_ from the start it's more appealing than learning what a tensor is. From the very start the course takes you through the whole process of developing an AI model and into deployment. I believe when the first version of the course was published AI was just beginning to be popular and many of the libraries and resources were _not yet invented_. Now they have all these cool tools to present the entire lifecycle while the students can better see the big picture.  This is just a testament of how much the community has evolved and how the _fastai_ team has succesfully managed to keep up the pace!\n",
    "\n",
    "fastai through the voice of Rachel Thomas encourages its students to start blogging. Another uncommon idea. What learning deep learning has anything to do with writing? [Here](https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045) you can find some good points about why blogging helps. Through this blog post (and maybe others that will follow) I want to test their hypothesis. I just want to try it and see how it goes.\n",
    "\n",
    "So, without further ado let's get our hands dirty with some deep learning. \n",
    "\n",
    "![meme](https://github.com/andreeas26/my-notes/blob/master/images/fastpages_posts/dl_meme.jpg?raw=true)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Write a basic Learner\n",
    "In this section I'll explain how I created a **_Learner_** from scratch by following the 7 steps for developing any deep learning model. \n",
    "\n",
    "\n",
    "\n",
    "1.   Initialize the parameters of the network;\n",
    "2.   Predict on some input data;\n",
    "3.   Compute the loss function. In other words how the predictions are different from the target (what we actually want);\n",
    "4.   Compute the parameters' gradients;\n",
    "5.   Update the parameters using backpropagation;\n",
    "6.   Repeat\n",
    "7.   Stop\n",
    "\n",
    "\n",
    "For this purpose I'll use the same MNIST_SAMPLE dataset that it's used in the book. It only contains the digits three and seven, so we'll start simple by resolving a binary classification problem. The fastai library provides some useful and easy to understand functions in order to get up and going realy quickly. I will not go into details about them. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "labels_df = pd.read_csv(path/\"labels.csv\")\n",
    "labels_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "(path/\"train\").ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def load_img(img_path):\n",
    "  img = Image.open(path/img_path)\n",
    "  # transform the image into a vector and rescale it to [0, 1]\n",
    "  img = tensor(img).view(28*28).float()/255 \n",
    "  return img\n",
    "\n",
    "train_df = labels_df.loc[labels_df['name'].str.contains('train'), :]\n",
    "test_df = labels_df.loc[labels_df['name'].str.contains('valid'), :]\n",
    "\n",
    "train_dst = [(load_img(row['name']), tensor(row['label'])) for _, row in train_df.iterrows()]\n",
    "test_dst  = [(load_img(row['name']), tensor(row['label'])) for _, row in test_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "train_dst[0][0].shape, train_dst[0][1].shape, test_dst[0][0].shape, test_dst[0][1].shape"
   ]
  },
  {
   "source": [
    "This is my implementation of a basic Learner that puts together all the necessary \"ingredients\" in order to train simple neural network:\n",
    "- `dls`: the data\n",
    "- `model`: the neural network\n",
    "- `opt_func`: optimization rule\n",
    "- `loss_func`: it's used by the model\n",
    "- `metrics`: the metrics we care about\n",
    "\n",
    "The function that does all the work is `fit()`. It updates the parameters of the network a number of `epochs` by geting a batch of input - target pairs from the train DataLoader (specified in `dls` parameter). The predictions from each batch are compared with the targets and based on these differences the gradiends are updated with a step set by `lr`. The last for loop it's for validation, where we are intereseted in the model performance on unseen data and its parameters are **not** updated. We're also do some brief logging after each epoch ends."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLearner:\n",
    "  __name__ = 'BasicLearner'\n",
    "  __repr__ = basic_repr('dls,model,opt_func,loss_func,metrics')\n",
    "\n",
    "  def __init__(self, dls, model, opt_func, loss_func, metrics):\n",
    "    store_attr('dls,model,opt_func,loss_func,metrics')\n",
    "  \n",
    "  def _reset(self):\n",
    "    self.train_loss = []\n",
    "    self.val_loss = []\n",
    "\n",
    "  def fit(self, epochs=10, lr=1e-2):\n",
    "    opt = self.opt_func(self.model.parameters(), lr=lr)\n",
    "    self._reset()\n",
    "\n",
    "    for e in range(epochs):\n",
    "      batch_train_loss = []\n",
    "      for x,y in self.dls.train:\n",
    "        pred = self.model(x)\n",
    "        loss = self.loss_func(pred, y)\n",
    "        batch_train_loss.append(loss)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "      batch_train_loss = tensor(batch_train_loss)\n",
    "\n",
    "      batch_val_loss = []\n",
    "      val_metrics = []\n",
    "      with torch.no_grad():\n",
    "        for x,y in self.dls.valid:\n",
    "          pred = self.model(x)\n",
    "          loss = self.loss_func(pred, y)\n",
    "          batch_val_loss.append(loss)\n",
    "          m = self.metrics(pred, y)\n",
    "          val_metrics.append(m)\n",
    "\n",
    "      batch_val_loss = tensor(batch_val_loss)\n",
    "      val_metrics = tensor(val_metrics)\n",
    "\n",
    "      self.train_loss.append(batch_train_loss.mean())\n",
    "      self.val_loss.append(batch_val_loss.mean())\n",
    "\n",
    "      msg = f\"Epoch {e}/{epochs}: \" \\\n",
    "        f\"Train loss {batch_train_loss.mean():.4f} \" \\\n",
    "        f\"Valid loss {batch_val_loss.mean():.4f} \" \\\n",
    "        f\"{self.metrics.__name__} {val_metrics.mean():.2f}\"\n",
    "      print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "bs = 128\n",
    "train_dl = DataLoader(train_dst, batch_size=bs)\n",
    "test_dl = DataLoader(test_dst, batch_size=bs, shuffle=False)\n",
    "dls = DataLoaders(train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class Linear:\n",
    "  __name__ = \"Linear\"\n",
    "  __repr__ = basic_repr('size,w,b')\n",
    "\n",
    "  def __init__(self, size):\n",
    "    self.w, self.b = self._init_params(size), self._init_params(size[1])\n",
    "    store_attr('size')\n",
    "\n",
    "  def _init_params(self, size, std=1.0):  \n",
    "    return (torch.randn(size, dtype=torch.float32)*std).requires_grad_()\n",
    "\n",
    "  def __call__(self, x):\n",
    "    res = x@self.w + self.b\n",
    "\n",
    "    return res\n",
    "  \n",
    "  def get_parameters(self):\n",
    "    return [self.w, self.b]\n",
    "\n",
    "class Basic2LayersNet:\n",
    "  __name__ = \"Basic2LayersNet\"\n",
    "  __repr__ = basic_repr('l1_size,l2_size,sigmoid')\n",
    "\n",
    "  def __init__(self, l1_size, l2_size, sigmoid=True):\n",
    "    store_attr('l1_size,l2_size,sigmoid')\n",
    "\n",
    "    self.l1 = Linear(l1_size)\n",
    "    self.l2 = Linear(l2_size)\n",
    "\n",
    "    self.sigmoid = sigmoid\n",
    "\n",
    "  def __call__(self, xb):\n",
    "    res = self.l1(xb)\n",
    "    res = res.max(tensor(0.0)) #ReLU \n",
    "    res = self.l2(res)\n",
    "\n",
    "    if sigmoid:\n",
    "      return res.sigmoid()\n",
    "    return res\n",
    "  \n",
    "  def parameters(self):\n",
    "    params = []\n",
    "    params += self.l1.get_parameters()\n",
    "    params += self.l2.get_parameters()\n",
    "    for p in params:\n",
    "      yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class BasicOptim:\n",
    "  __name__ = \"BasicOptim\"\n",
    "  __repr__ = basic_repr('parameters,lr')\n",
    "  def __init__(self, parameters, lr): self.parameters, self.lr = list(parameters), lr\n",
    "  \n",
    "  def step(self, *args, **kwargs):\n",
    "    for p in self.parameters: p.data -= p.grad * self.lr\n",
    "  \n",
    "  def zero_grad(self, *args, **kwargs):\n",
    "    for p in self.parameters: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
    "\n",
    "def mnist_accuracy(predictions, targets):\n",
    "    correct = (predictions > 0.5) == targets\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "basic_net = Basic2LayersNet(l1_size=(28*28, 30), l2_size=(30, 1))\n",
    "basic_net"
   ]
  },
  {
   "source": [
    "Let's see how it works on the MNIST_SAMPLE dataset. You can check the implemention of all the parameters given to `BasicLearner` class, on github.\n",
    "The basic network will train for 20 epochs with a learning rate of 0.1. After just a few epochs the model has over 90% accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BasicLearner(dls, basic_net, BasicOptim, mnist_loss, mnist_accuracy)\n",
    "print(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(20, lr=0.1)"
   ]
  }
 ]
}