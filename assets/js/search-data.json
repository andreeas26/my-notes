{
  
    
        "post0": {
            "title": "A basic Learner",
            "content": "Intro . I have recently started the fastai course v4 after many postponements. Although I have been working with AI for a few years now I always thought Jeremy Howard&#39;s approach of teaching it&#39;s very interesting and ... different. Learning the whole game from the start it&#39;s more appealing than learning what a tensor is. From the very start the course takes you through the whole process of developing an AI model and into deployment. I believe when the first version of the course was published AI was just beginning to be popular and many of the libraries and resources were not yet invented. Now they have all these cool tools to present the entire lifecycle while the students can better see the big picture. This is just a testament of how much the community has evolved and how the fastai team has succesfully managed to keep up the pace! . fastai through the voice of Rachel Thomas encourages its students to start blogging. Another uncommon idea. What learning deep learning has anything to do with writing? Here you can find some good points about why blogging helps. Through this blog post (and maybe others that will follow) I want to test their hypothesis. I just want to try it and see how it goes. . So, without further ado let&#39;s get our hands dirty with some deep learning. . . Write a basic Learner . In this section I&#39;ll explain how I created a Learner from scratch by following the 7 steps for developing any deep learning model. . Initialize the parameters of the network; | Predict on some input data; | Compute the loss function. In other words how the predictions are different from the target (what we actually want); | Compute the parameters&#39; gradients; | Update the parameters using backpropagation; | Repeat | Stop | For this purpose I&#39;ll use the same MNIST_SAMPLE dataset that it&#39;s used in the book. It only contains the digits three and seven, so we&#39;ll start simple by resolving a binary classification problem. The fastai library provides some useful and easy to understand functions in order to get up and going realy quickly. I will not go into details about them. . This is my implementation of a basic Learner that puts together all the necessary &quot;ingredients&quot; in order to train simple neural network: . dls: the data | model: the neural network | opt_func: optimization rule | loss_func: it&#39;s used by the model | metrics: the metrics we care about | . The function that does all the work is fit(). It updates the parameters of the network a number of epochs by geting a batch of input - target pairs from the train DataLoader (specified in dls parameter). The predictions from each batch are compared with the targets and based on these differences the gradiends are updated with a step set by lr. The last for loop it&#39;s for validation, where we are intereseted in the model performance on unseen data and its parameters are not updated. We&#39;re also do some brief logging after each epoch ends. . class BasicLearner: __name__ = &#39;BasicLearner&#39; __repr__ = basic_repr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def __init__(self, dls, model, opt_func, loss_func, metrics): store_attr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def _reset(self): self.train_loss = [] self.val_loss = [] def fit(self, epochs=10, lr=1e-2): opt = self.opt_func(self.model.parameters(), lr=lr) self._reset() for e in range(epochs): batch_train_loss = [] for x,y in self.dls.train: pred = self.model(x) loss = self.loss_func(pred, y) batch_train_loss.append(loss) opt.zero_grad() loss.backward() opt.step() batch_train_loss = tensor(batch_train_loss) batch_val_loss = [] val_metrics = [] with torch.no_grad(): for x,y in self.dls.valid: pred = self.model(x) loss = self.loss_func(pred, y) batch_val_loss.append(loss) m = self.metrics(pred, y) val_metrics.append(m) batch_val_loss = tensor(batch_val_loss) val_metrics = tensor(val_metrics) self.train_loss.append(batch_train_loss.mean()) self.val_loss.append(batch_val_loss.mean()) msg = f&quot;Epoch {e}/{epochs}: &quot; f&quot;Train loss {batch_train_loss.mean():.4f} &quot; f&quot;Valid loss {batch_val_loss.mean():.4f} &quot; f&quot;{self.metrics.__name__} {val_metrics.mean():.2f}&quot; print(msg) . Let&#39;s see how it works on the MNIST_SAMPLE dataset. You can check the implemention of all the parameters given to BasicLearner class, on github. The basic network will train for 20 epochs with a learning rate of 0.1. After just a few epochs the model has over 90% accuracy. . learn = BasicLearner(dls, basic_net, BasicOptim, mnist_loss, mnist_accuracy) print(learn) . learn.fit(20, lr=0.1) .",
            "url": "https://andreeas26.github.io/my-notes/fastpages/jupyter/learner/machine%20learning/2021/03/19/basic-learner.html",
            "relUrl": "/fastpages/jupyter/learner/machine%20learning/2021/03/19/basic-learner.html",
            "date": " • Mar 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A basic Learner",
            "content": "Intro . I have recently started the fastai course v4 after many postponements. Although I have been working with AI for a few years now I always thought Jeremy Howard&#39;s approach of teaching it&#39;s very interesting and ... different. Learning the whole game from the start it&#39;s more appealing than learning what a tensor is. From the very start the course takes you through the whole process of developing an AI model and into deployment. I believe when the first version of the course was published AI was just beginning to be popular and many of the libraries and resources were not yet invented. Now they have all these cool tools to present the entire lifecycle while the students can better see the big picture. This is just a testament of how much the community has evolved and how the fastai team has succesfully managed to keep up the pace! . fastai through the voice of Rachel Thomas encourages its students to start blogging. Another uncommon idea. What learning deep learning has anything to do with writing? Here you can find some good points about why blogging helps. Through this blog post (and maybe others that will follow) I want to test their hypothesis. I just want to try it and see how it goes. . So, without further ado let&#39;s get our hands dirty with some deep learning. . . Write a basic Learner . In this section I&#39;ll explain how I created a Learner from scratch by following the 7 steps for developing any deep learning model. . Initialize the parameters of the network; | Predict on some input data; | Compute the loss function. In other words how the predictions are different from the target (what we actually want); | Compute the parameters&#39; gradients; | Update the parameters using backpropagation; | Repeat | Stop | For this purpose I&#39;ll use the same MNIST_SAMPLE dataset that it&#39;s used in the book. It only contains the digits three and seven, so we&#39;ll start simple by resolving a binary classification problem. The fastai library provides some useful and easy to understand functions in order to get up and going realy quickly. I will not go into details about them. . path = untar_data(URLs.MNIST_SAMPLE) path.ls() . (#3) [Path(&#39;C:/Users/andreea.sandu/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;C:/Users/andreea.sandu/.fastai/data/mnist_sample/train&#39;),Path(&#39;C:/Users/andreea.sandu/.fastai/data/mnist_sample/valid&#39;)] . labels_df = pd.read_csv(path/&quot;labels.csv&quot;) labels_df.head(3) . name label . 0 train/3/7463.png | 0 | . 1 train/3/21102.png | 0 | . 2 train/3/31559.png | 0 | . (path/&quot;train&quot;).ls() . (#2) [Path(&#39;C:/Users/andreea.sandu/.fastai/data/mnist_sample/train/3&#39;),Path(&#39;C:/Users/andreea.sandu/.fastai/data/mnist_sample/train/7&#39;)] . def load_img(img_path): img = Image.open(path/img_path) # transform the image into a vector and rescale it to [0, 1] img = tensor(img).view(28*28).float()/255 return img train_df = labels_df.loc[labels_df[&#39;name&#39;].str.contains(&#39;train&#39;), :] test_df = labels_df.loc[labels_df[&#39;name&#39;].str.contains(&#39;valid&#39;), :] train_dst = [(load_img(row[&#39;name&#39;]), tensor(row[&#39;label&#39;])) for _, row in train_df.iterrows()] test_dst = [(load_img(row[&#39;name&#39;]), tensor(row[&#39;label&#39;])) for _, row in test_df.iterrows()] . train_dst[0][0].shape, train_dst[0][1].shape, test_dst[0][0].shape, test_dst[0][1].shape . (torch.Size([784]), torch.Size([]), torch.Size([784]), torch.Size([])) . This is my implementation of a basic Learner that puts together all the necessary &quot;ingredients&quot; in order to train simple neural network: . dls: the data | model: the neural network | opt_func: optimization rule | loss_func: it&#39;s used by the model | metrics: the metrics we care about | . The function that does all the work is fit(). It updates the parameters of the network a number of epochs by geting a batch of input - target pairs from the train DataLoader (specified in dls parameter). The predictions from each batch are compared with the targets and based on these differences the gradiends are updated with a step set by lr. The last for loop it&#39;s for validation, where we are intereseted in the model performance on unseen data and its parameters are not updated. We&#39;re also do some brief logging after each epoch ends. . class BasicLearner: __name__ = &#39;BasicLearner&#39; __repr__ = basic_repr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def __init__(self, dls, model, opt_func, loss_func, metrics): store_attr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def _reset(self): self.train_loss = [] self.val_loss = [] def fit(self, epochs=10, lr=1e-2): opt = self.opt_func(self.model.parameters(), lr=lr) self._reset() for e in range(epochs): batch_train_loss = [] for x,y in self.dls.train: pred = self.model(x) loss = self.loss_func(pred, y) batch_train_loss.append(loss) opt.zero_grad() loss.backward() opt.step() batch_train_loss = tensor(batch_train_loss) batch_val_loss = [] val_metrics = [] with torch.no_grad(): for x,y in self.dls.valid: pred = self.model(x) loss = self.loss_func(pred, y) batch_val_loss.append(loss) m = self.metrics(pred, y) val_metrics.append(m) batch_val_loss = tensor(batch_val_loss) val_metrics = tensor(val_metrics) self.train_loss.append(batch_train_loss.mean()) self.val_loss.append(batch_val_loss.mean()) msg = f&quot;Epoch {e}/{epochs}: &quot; f&quot;Train loss {batch_train_loss.mean():.4f} &quot; f&quot;Valid loss {batch_val_loss.mean():.4f} &quot; f&quot;{self.metrics.__name__} {val_metrics.mean():.2f}&quot; print(msg) . bs = 128 train_dl = DataLoader(train_dst, batch_size=bs) test_dl = DataLoader(test_dst, batch_size=bs, shuffle=False) dls = DataLoaders(train_dl, test_dl) . class Linear: __name__ = &quot;Linear&quot; __repr__ = basic_repr(&#39;size,w,b&#39;) def __init__(self, size): self.w, self.b = self._init_params(size), self._init_params(size[1]) store_attr(&#39;size&#39;) def _init_params(self, size, std=1.0): return (torch.randn(size, dtype=torch.float32)*std).requires_grad_() def __call__(self, x): res = x@self.w + self.b return res def get_parameters(self): return [self.w, self.b] class Basic2LayersNet: __name__ = &quot;Basic2LayersNet&quot; __repr__ = basic_repr(&#39;l1_size,l2_size,sigmoid&#39;) def __init__(self, l1_size, l2_size, sigmoid=True): store_attr(&#39;l1_size,l2_size,sigmoid&#39;) self.l1 = Linear(l1_size) self.l2 = Linear(l2_size) self.sigmoid = sigmoid def __call__(self, xb): res = self.l1(xb) res = res.max(tensor(0.0)) #ReLU res = self.l2(res) if sigmoid: return res.sigmoid() return res def parameters(self): params = [] params += self.l1.get_parameters() params += self.l2.get_parameters() for p in params: yield p . class BasicOptim: __name__ = &quot;BasicOptim&quot; __repr__ = basic_repr(&#39;parameters,lr&#39;) def __init__(self, parameters, lr): self.parameters, self.lr = list(parameters), lr def step(self, *args, **kwargs): for p in self.parameters: p.data -= p.grad * self.lr def zero_grad(self, *args, **kwargs): for p in self.parameters: p.grad = None . def mnist_loss(predictions, targets): return torch.where(targets==1, 1-predictions, predictions).mean() def mnist_accuracy(predictions, targets): correct = (predictions &gt; 0.5) == targets return correct.float().mean() . basic_net = Basic2LayersNet(l1_size=(28*28, 30), l2_size=(30, 1)) basic_net . Basic2LayersNet(l1_size=(784, 30), l2_size=(30, 1), sigmoid=True) . Let&#39;s see how it works on the MNIST_SAMPLE dataset. You can check the implemention of all the parameters given to BasicLearner class, on github. The basic network will train for 20 epochs with a learning rate of 0.1. After just a few epochs the model has over 90% accuracy. . learn = BasicLearner(dls, basic_net, BasicOptim, mnist_loss, mnist_accuracy) print(learn) . BasicLearner(dls=&lt;fastai.data.core.DataLoaders object at 0x000001A1C4F52BA8&gt;, model=Basic2LayersNet(l1_size=(784, 30), l2_size=(30, 1), sigmoid=True), opt_func=&lt;class &#39;__main__.BasicOptim&#39;&gt;, loss_func=&lt;function mnist_loss at 0x000001A1C4F57378&gt;, metrics=&lt;function mnist_accuracy at 0x000001A1C4F57B70&gt;) . learn.fit(20, lr=0.1) . Epoch 0/20: Train loss 0.2879 Valid loss 0.3009 mnist_accuracy 0.70 Epoch 1/20: Train loss 0.1435 Valid loss 0.2375 mnist_accuracy 0.76 Epoch 2/20: Train loss 0.1138 Valid loss 0.1562 mnist_accuracy 0.85 Epoch 3/20: Train loss 0.0952 Valid loss 0.1114 mnist_accuracy 0.89 Epoch 4/20: Train loss 0.0795 Valid loss 0.0899 mnist_accuracy 0.91 Epoch 5/20: Train loss 0.0661 Valid loss 0.0755 mnist_accuracy 0.92 Epoch 6/20: Train loss 0.0603 Valid loss 0.0662 mnist_accuracy 0.93 Epoch 7/20: Train loss 0.0563 Valid loss 0.0619 mnist_accuracy 0.94 Epoch 8/20: Train loss 0.0524 Valid loss 0.0590 mnist_accuracy 0.94 Epoch 9/20: Train loss 0.0483 Valid loss 0.0559 mnist_accuracy 0.94 Epoch 10/20: Train loss 0.0468 Valid loss 0.0530 mnist_accuracy 0.95 Epoch 11/20: Train loss 0.0451 Valid loss 0.0513 mnist_accuracy 0.95 Epoch 12/20: Train loss 0.0435 Valid loss 0.0503 mnist_accuracy 0.95 Epoch 13/20: Train loss 0.0417 Valid loss 0.0497 mnist_accuracy 0.95 Epoch 14/20: Train loss 0.0401 Valid loss 0.0492 mnist_accuracy 0.95 Epoch 15/20: Train loss 0.0393 Valid loss 0.0486 mnist_accuracy 0.95 Epoch 16/20: Train loss 0.0379 Valid loss 0.0480 mnist_accuracy 0.95 Epoch 17/20: Train loss 0.0365 Valid loss 0.0471 mnist_accuracy 0.95 Epoch 18/20: Train loss 0.0358 Valid loss 0.0465 mnist_accuracy 0.95 Epoch 19/20: Train loss 0.0354 Valid loss 0.0461 mnist_accuracy 0.95 .",
            "url": "https://andreeas26.github.io/my-notes/fastpages/jupyter/learner/machine%20learning/2021/03/02/basic-learner.html",
            "relUrl": "/fastpages/jupyter/learner/machine%20learning/2021/03/02/basic-learner.html",
            "date": " • Mar 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://andreeas26.github.io/my-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://andreeas26.github.io/my-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}