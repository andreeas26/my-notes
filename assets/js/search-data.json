{
  
    
        "post0": {
            "title": "A basic Learner",
            "content": "Intro . I have recently started the fastai course v4 after many postponements. Although I have been working with AI for a few years now I always thought Jeremy Howard&#39;s approach of teaching it&#39;s very interesting and ... different. Learning the whole game from the start it&#39;s more appealing than learning what a tensor is. From the very start the course takes you through the whole process of developing an AI model and into deployment. I believe when the first version of the course was published AI was just beginning to be popular and many of the libraries and resources were not yet invented. Now they have all these cool tools to present the entire lifecycle while the students can better see the big picture. This is just a testament of how much the community has evolved and how the fastai team has succesfully managed to keep up the pace! . fastai through the voice of Rachel Thomas encourages its students to start blogging. Another uncommon idea. What learning deep learning has anything to do with writing? Here you can find some good points about why blogging helps. Through this blog post (and maybe others that will follow) I want to test their hypothesis. I just want to try it and see how it goes. . So, without further ado let&#39;s get our hands dirty with some deep learning. . . Write a basic Learner . In this section I&#39;ll explain how I created a Learner from scratch by following the 7 steps for developing any deep learning model. . Initialize the parameters of the network; | Predict on some input data; | Compute the loss function. In other words how the predictions are different from the target (what we actually want); | Compute the parameters&#39; gradients; | Update the parameters using backpropagation; | Repeat | Stop | For this purpose I&#39;ll use the same MNIST_SAMPLE dataset that it&#39;s used in the book. It only contains the digits three and seven, so we&#39;ll start simple by resolving a binary classification problem. The fastai library provides some useful and easy to understand functions in order to get up and going realy quickly. I will not go into details about them. . This is my implementation of a basic Learner that puts together all the necessary &quot;ingredients&quot; in order to train simple neural network: . dls: the data | model: the neural network | opt_func: optimization rule | loss_func: it&#39;s used by the model | metrics: the metrics we care about | . The function that does all the work is fit(). It updates the parameters of the network a number of epochs by geting a batch of input - target pairs from the train DataLoader (specified in dls parameter). The predictions from each batch are compared with the targets and based on these differences the gradiends are updated with a step set by lr. The last for loop it&#39;s for validation, where we are intereseted in the model performance on unseen data and its parameters are not updated. We&#39;re also do some brief logging after each epoch ends. . class BasicLearner: __name__ = &#39;BasicLearner&#39; __repr__ = basic_repr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def __init__(self, dls, model, opt_func, loss_func, metrics): store_attr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def _reset(self): self.train_loss = [] self.val_loss = [] def fit(self, epochs=10, lr=1e-2): opt = self.opt_func(self.model.parameters(), lr=lr) self._reset() for e in range(epochs): batch_train_loss = [] for x,y in self.dls.train: pred = self.model(x) loss = self.loss_func(pred, y) batch_train_loss.append(loss) opt.zero_grad() loss.backward() opt.step() batch_train_loss = tensor(batch_train_loss) batch_val_loss = [] val_metrics = [] with torch.no_grad(): for x,y in self.dls.valid: pred = self.model(x) loss = self.loss_func(pred, y) batch_val_loss.append(loss) m = self.metrics(pred, y) val_metrics.append(m) batch_val_loss = tensor(batch_val_loss) val_metrics = tensor(val_metrics) self.train_loss.append(batch_train_loss.mean()) self.val_loss.append(batch_val_loss.mean()) msg = f&quot;Epoch {e}/{epochs}: &quot; f&quot;Train loss {batch_train_loss.mean():.4f} &quot; f&quot;Valid loss {batch_val_loss.mean():.4f} &quot; f&quot;{self.metrics.__name__} {val_metrics.mean():.2f}&quot; print(msg) . Let&#39;s see how it works on the MNIST_SAMPLE dataset. You can check the implemention of all the parameters given to BasicLearner class, on github. The basic network will train for 20 epochs with a learning rate of 0.1. After just a few epochs the model has over 90% accuracy. . learn = BasicLearner(dls, basic_net, BasicOptim, mnist_loss, mnist_accuracy) print(learn) . BasicLearner(dls=&lt;fastai.data.core.DataLoaders object at 0x00000214F33F9D68&gt;, model=Basic2LayersNet(l1_size=(784, 30), l2_size=(30, 1), sigmoid=True), opt_func=&lt;class &#39;__main__.BasicOptim&#39;&gt;, loss_func=&lt;function mnist_loss at 0x00000214F334FB70&gt;, metrics=&lt;function mnist_accuracy at 0x00000214F7B0D0D0&gt;) . learn.fit(20, lr=0.1) . Epoch 0/20: Train loss 0.2138 Valid loss 0.3241 mnist_accuracy 0.67 Epoch 1/20: Train loss 0.1307 Valid loss 0.1945 mnist_accuracy 0.81 Epoch 2/20: Train loss 0.0980 Valid loss 0.1210 mnist_accuracy 0.88 Epoch 3/20: Train loss 0.0814 Valid loss 0.0959 mnist_accuracy 0.90 Epoch 4/20: Train loss 0.0663 Valid loss 0.0856 mnist_accuracy 0.91 Epoch 5/20: Train loss 0.0612 Valid loss 0.0749 mnist_accuracy 0.93 Epoch 6/20: Train loss 0.0572 Valid loss 0.0707 mnist_accuracy 0.93 Epoch 7/20: Train loss 0.0519 Valid loss 0.0668 mnist_accuracy 0.93 Epoch 8/20: Train loss 0.0490 Valid loss 0.0639 mnist_accuracy 0.94 Epoch 9/20: Train loss 0.0463 Valid loss 0.0603 mnist_accuracy 0.94 Epoch 10/20: Train loss 0.0440 Valid loss 0.0567 mnist_accuracy 0.94 Epoch 11/20: Train loss 0.0424 Valid loss 0.0539 mnist_accuracy 0.95 Epoch 12/20: Train loss 0.0408 Valid loss 0.0524 mnist_accuracy 0.95 Epoch 13/20: Train loss 0.0393 Valid loss 0.0511 mnist_accuracy 0.95 Epoch 14/20: Train loss 0.0383 Valid loss 0.0493 mnist_accuracy 0.95 Epoch 15/20: Train loss 0.0375 Valid loss 0.0475 mnist_accuracy 0.95 Epoch 16/20: Train loss 0.0365 Valid loss 0.0458 mnist_accuracy 0.95 Epoch 17/20: Train loss 0.0355 Valid loss 0.0445 mnist_accuracy 0.96 Epoch 18/20: Train loss 0.0345 Valid loss 0.0437 mnist_accuracy 0.96 Epoch 19/20: Train loss 0.0336 Valid loss 0.0430 mnist_accuracy 0.96 .",
            "url": "https://andreeas26.github.io/my-notes/fastpages/jupyter/learner/machine%20learning/2021/03/19/basic-learner.html",
            "relUrl": "/fastpages/jupyter/learner/machine%20learning/2021/03/19/basic-learner.html",
            "date": " • Mar 19, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://andreeas26.github.io/my-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://andreeas26.github.io/my-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}