{
  
    
        "post0": {
            "title": "A basic Learner",
            "content": "Intro . I have recently started the fastai course v4 after many postponements. Although I have been working with AI for a few years now I always thought Jeremy Howard&#39;s approach of teaching it&#39;s very interesting and ... different. Learning the whole game from the start it&#39;s more appealing than learning what a tensor is. From the very start the course takes you through the whole process of developing an AI model and into deployment. I believe when the first version of the course was published AI was just beginning to be popular and many of the libraries and resources were not yet invented. Now they have all these cool tools to present the entire lifecycle while the students can better see the big picture. This is just a testament of how much the community has evolved and how the fastai team has succesfully managed to keep up the pace! . fastai through the voice of Rachel Thomas encourages its students to start blogging. Another uncommon idea. What learning deep learning has anything to do with writing? Here you can find some good points about why blogging helps. Through this blog post (and maybe others that will follow) I want to test their hypothesis. I just want to try it and see how it goes. . So, without further ado let&#39;s get our hands dirty with some deep learning. . . Write a basic Learner . In this section I&#39;ll explain how I created a Learner from scratch by following the 7 steps for developing any deep learning model. . Initialize the parameters of the network; | Predict on some input data; | Compute the loss function. In other words how the predictions are different from the target (what we actually want); | Compute the parameters&#39; gradients; | Update the parameters using backpropagation; | Repeat | Stop | For this purpose I&#39;ll use the same MNIST_SAMPLE dataset that it&#39;s used in the book. It only contains the digits three and seven, so we&#39;ll start simple by resolving a binary classification problem. The fastai library provides some useful and easy to understand functions in order to get up and going realy quickly. I will not go into details about them. . !pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastai.vision.all import * from fastbook import * import pandas as pd . path = untar_data(URLs.MNIST_SAMPLE) path.ls() . (#3) [Path(&#39;/root/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;/root/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/root/.fastai/data/mnist_sample/train&#39;)] . labels_df = pd.read_csv(path/&quot;labels.csv&quot;) labels_df.head(3) . name label . 0 train/3/7463.png | 0 | . 1 train/3/21102.png | 0 | . 2 train/3/31559.png | 0 | . (path/&quot;train&quot;).ls() . (#2) [Path(&#39;/root/.fastai/data/mnist_sample/train/7&#39;),Path(&#39;/root/.fastai/data/mnist_sample/train/3&#39;)] . def load_img(img_path): img = Image.open(path/img_path) # transform the image into a vector and rescale it to [0, 1] img = tensor(img).view(28*28).float()/255 return img train_df = labels_df.loc[labels_df[&#39;name&#39;].str.contains(&#39;train&#39;), :] test_df = labels_df.loc[labels_df[&#39;name&#39;].str.contains(&#39;valid&#39;), :] train_dst = [(load_img(row[&#39;name&#39;]), tensor(row[&#39;label&#39;])) for _, row in train_df.iterrows()] test_dst = [(load_img(row[&#39;name&#39;]), tensor(row[&#39;label&#39;])) for _, row in test_df.iterrows()] . train_dst[0][0].shape, train_dst[0][1].shape, test_dst[0][0].shape, test_dst[0][1].shape . (torch.Size([784]), torch.Size([]), torch.Size([784]), torch.Size([])) . This is my implementation of a basic Learner that puts together all the necessary &quot;ingredients&quot; in order to train simple neural network: . dls: the data | model: the neural network | opt_func: optimization rule | loss_func: it&#39;s used by the model | metrics: the metrics we care about | . The function that does all the work is fit(). It updates the parameters of the network a number of epochs by geting a batch of input - target pairs from the train DataLoader (specified in dls parameter). The predictions from each batch are compared with the targets and based on these differences the gradiends are updated with a step set by lr. The last for loop it&#39;s for validation, where we are intereseted in the model performance on unseen data and its parameters are not updated. We&#39;re also do some brief logging after each epoch ends. . class BasicLearner: __name__ = &#39;BasicLearner&#39; __repr__ = basic_repr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def __init__(self, dls, model, opt_func, loss_func, metrics): store_attr(&#39;dls,model,opt_func,loss_func,metrics&#39;) def _reset(self): self.train_loss = [] self.val_loss = [] def fit(self, epochs=10, lr=1e-2): opt = self.opt_func(self.model.parameters(), lr=lr) self._reset() for e in range(epochs): batch_train_loss = [] for x,y in self.dls.train: pred = self.model(x) loss = self.loss_func(pred, y) batch_train_loss.append(loss) opt.zero_grad() loss.backward() opt.step() batch_train_loss = tensor(batch_train_loss) batch_val_loss = [] val_metrics = [] with torch.no_grad(): for x,y in self.dls.valid: pred = self.model(x) loss = self.loss_func(pred, y) batch_val_loss.append(loss) m = self.metrics(pred, y) val_metrics.append(m) batch_val_loss = tensor(batch_val_loss) val_metrics = tensor(val_metrics) self.train_loss.append(batch_train_loss.mean()) self.val_loss.append(batch_val_loss.mean()) msg = f&quot;Epoch {e}/{epochs}: &quot; f&quot;Train loss {batch_train_loss.mean():.4f} &quot; f&quot;Valid loss {batch_val_loss.mean():.4f} &quot; f&quot;{self.metrics.__name__} {val_metrics.mean():.2f}&quot; print(msg) . bs = 128 train_dl = DataLoader(train_dst, batch_size=bs) test_dl = DataLoader(test_dst, batch_size=bs, shuffle=False) dls = DataLoaders(train_dl, test_dl) . class Linear: __name__ = &quot;Linear&quot; __repr__ = basic_repr(&#39;size,w,b&#39;) def __init__(self, size): self.w, self.b = self._init_params(size), self._init_params(size[1]) store_attr(&#39;size&#39;) def _init_params(self, size, std=1.0): return (torch.randn(size, dtype=torch.float32)*std).requires_grad_() def __call__(self, x): res = x@self.w + self.b return res def get_parameters(self): return [self.w, self.b] class Basic2LayersNet: __name__ = &quot;Basic2LayersNet&quot; __repr__ = basic_repr(&#39;l1_size,l2_size,sigmoid&#39;) def __init__(self, l1_size, l2_size, sigmoid=True): store_attr(&#39;l1_size,l2_size,sigmoid&#39;) self.l1 = Linear(l1_size) self.l2 = Linear(l2_size) self.sigmoid = sigmoid def __call__(self, xb): res = self.l1(xb) res = res.max(tensor(0.0)) #ReLU res = self.l2(res) if sigmoid: return res.sigmoid() return res def parameters(self): params = [] params += self.l1.get_parameters() params += self.l2.get_parameters() for p in params: yield p . class BasicOptim: __name__ = &quot;BasicOptim&quot; __repr__ = basic_repr(&#39;parameters,lr&#39;) def __init__(self, parameters, lr): self.parameters, self.lr = list(parameters), lr def step(self, *args, **kwargs): for p in self.parameters: p.data -= p.grad * self.lr def zero_grad(self, *args, **kwargs): for p in self.parameters: p.grad = None . def mnist_loss(predictions, targets): return torch.where(targets==1, 1-predictions, predictions).mean() def mnist_accuracy(predictions, targets): correct = (predictions &gt; 0.5) == targets return correct.float().mean() . basic_net = Basic2LayersNet(l1_size=(28*28, 30), l2_size=(30, 1)) basic_net . Basic2LayersNet(l1_size=(784, 30), l2_size=(30, 1), sigmoid=True) . Let&#39;s see how it works on the MNIST_SAMPLE dataset. You can check the implemention of all the parameters given to BasicLearner class, on github. The basic network will train for 20 epochs with a learning rate of 0.1. After just a few epochs the model has over 90% accuracy. . learn = BasicLearner(dls, basic_net, BasicOptim, mnist_loss, mnist_accuracy) print(learn) . BasicLearner(dls=&lt;fastai.data.core.DataLoaders object at 0x7f2c1c21e1d0&gt;, model=Basic2LayersNet(l1_size=(784, 30), l2_size=(30, 1), sigmoid=True), opt_func=&lt;class &#39;__main__.BasicOptim&#39;&gt;, loss_func=&lt;function mnist_loss at 0x7f2c1c2b9830&gt;, metrics=&lt;function mnist_accuracy at 0x7f2c1c2b95f0&gt;) . learn.fit(20, lr=0.1) . Epoch 0/20: Train loss 0.5973 Valid loss 0.5052 mnist_accuracy 0.50 Epoch 1/20: Train loss 0.5023 Valid loss 0.4883 mnist_accuracy 0.51 Epoch 2/20: Train loss 0.3698 Valid loss 0.4163 mnist_accuracy 0.58 Epoch 3/20: Train loss 0.1523 Valid loss 0.3092 mnist_accuracy 0.69 Epoch 4/20: Train loss 0.1131 Valid loss 0.1632 mnist_accuracy 0.84 Epoch 5/20: Train loss 0.0843 Valid loss 0.1107 mnist_accuracy 0.89 Epoch 6/20: Train loss 0.0750 Valid loss 0.0910 mnist_accuracy 0.91 Epoch 7/20: Train loss 0.0683 Valid loss 0.0794 mnist_accuracy 0.92 Epoch 8/20: Train loss 0.0631 Valid loss 0.0693 mnist_accuracy 0.93 Epoch 9/20: Train loss 0.0590 Valid loss 0.0643 mnist_accuracy 0.94 Epoch 10/20: Train loss 0.0570 Valid loss 0.0602 mnist_accuracy 0.94 Epoch 11/20: Train loss 0.0546 Valid loss 0.0568 mnist_accuracy 0.94 Epoch 12/20: Train loss 0.0513 Valid loss 0.0545 mnist_accuracy 0.94 Epoch 13/20: Train loss 0.0497 Valid loss 0.0524 mnist_accuracy 0.95 Epoch 14/20: Train loss 0.0491 Valid loss 0.0512 mnist_accuracy 0.95 Epoch 15/20: Train loss 0.0483 Valid loss 0.0504 mnist_accuracy 0.95 Epoch 16/20: Train loss 0.0473 Valid loss 0.0496 mnist_accuracy 0.95 Epoch 17/20: Train loss 0.0464 Valid loss 0.0488 mnist_accuracy 0.95 Epoch 18/20: Train loss 0.0453 Valid loss 0.0477 mnist_accuracy 0.95 Epoch 19/20: Train loss 0.0442 Valid loss 0.0464 mnist_accuracy 0.95 .",
            "url": "https://andreeas26.github.io/my-notes/fastpages/jupyter/learner/machine%20learning/2021/03/02/basic-learner.html",
            "relUrl": "/fastpages/jupyter/learner/machine%20learning/2021/03/02/basic-learner.html",
            "date": " • Mar 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://andreeas26.github.io/my-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://andreeas26.github.io/my-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}